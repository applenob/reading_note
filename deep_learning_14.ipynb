{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [0.书本介绍](https://applenob.github.io/deep_learning_1#0.书本介绍)\n",
    "- [1. Introduction](https://applenob.github.io/deep_learning_1#1.-Introduction)\n",
    "- [2. Linear Algebra](https://applenob.github.io/deep_learning_2)\n",
    "- [3. Probability and Information Theory](https://applenob.github.io/deep_learning_3)\n",
    "- [4. Numerical Computation](https://applenob.github.io/deep_learning_4)\n",
    "- [5. Machine Learning Basics](https://applenob.github.io/deep_learning_5)\n",
    "- [6. Deep Feedforward Networks](https://applenob.github.io/deep_learning_6)\n",
    "- [7. Regularization for Deep Learning](https://applenob.github.io/deep_learning_7)\n",
    "- [8. Optimization for Training Deep Models](https://applenob.github.io/deep_learning_8)\n",
    "- [9. Convolutional Networks](https://applenob.github.io/deep_learning_9)\n",
    "- [10. Sequence Modeling: Recurrent and Recursive Nets](https://applenob.github.io/deep_learning_10)\n",
    "- [11. Practical Methodology](https://applenob.github.io/deep_learning_11)\n",
    "- [12. Application](https://applenob.github.io/deep_learning_12)\n",
    "- [13. Linear Factor Models](https://applenob.github.io/deep_learning_13)\n",
    "- [14. Autoencoders](https://applenob.github.io/deep_learning_14)\n",
    "- [15. Representation Learning](https://applenob.github.io/deep_learning_15)\n",
    "- [16. Structured Probabilistic Models for Deep Learning](https://applenob.github.io/deep_learning_16)\n",
    "- [17. Monte Carlo Methods](https://applenob.github.io/deep_learning_17)\n",
    "- [18. Confronting the Partition Function](https://applenob.github.io/deep_learning_18)\n",
    "- [19. Approximate Inference](https://applenob.github.io/deep_learning_19)\n",
    "- [20. Deep Generative Models](https://applenob.github.io/deep_learning_20)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 14. Autoencoders\n",
    "\n",
    "**自编码器（Autoencoders）**：自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。 自编码器内部有一个隐藏层 $h$，产生编码表示输入。 该网络由两部分组成：一个由函数$h = f(x)$表示的编码器和一个生成重构的解码器 $r=g(h)$。 如果一个自编码器只是简单地学会将处处设置为$g(f(x)) = x$，那么这个自编码器就没什么特别的用处。 相反，我们不应该将自编码器设计成输入到输出完全相等:向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。 这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。 现代自编码器将编码器和解码器的概念推而广之，将确定函数推广为随机映射$p_{encoder} (h \\mid x)$和$p_{decoder}(x \\mid h)$。 所有自编码器的训练过程涉及两种推动力的折衷： 1.学习训练样本$x$的表示$h$使得$x$能通过解码器近似地从$h$中恢复，$x$是从训练数据挑出的这一事实很关键，因为这意味着在自编码器不需要成功重构不属于数据生成分布下的输入；2.满足约束或正则惩罚。 这可以是限制自编码器容量的架构约束，也可以是加入到重构代价的一个正则项。 两种推动力结合才是有用的，因为它们驱使隐藏的表示能捕获有关数据分布结构的信息。\n",
    "![](https://github.com/applenob/reading_note/raw/master/res/autoencoder.png)\n",
    "\n",
    "**自编码器的应用**：传统自编码器被用于**降维**或**特征学习**。 近年来，自编码器与潜变量模型理论的联系将自编码器带到了**生成建模**的前沿。\n",
    "\n",
    "**欠完备自编码器（Undercomplete Autoencoder）**：从自编码器获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。 损失函数$L(x,g(f(x)))$，惩罚$g(f(x))$与$x$的差异，如均方误差。 当解码器是线性的且$L$是均方误差，欠完备的自编码器会学习出与PCA相同的生成子空间。 不幸的是，如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。 \n",
    "\n",
    "**正则自编码器（Regularized Autoencoders）**：使用的损失函数可以鼓励模型学习**其他特性（除了将输入复制到输出）**，而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。 这些特性包括**稀疏表示**、**表示的小导数**、以及**对噪声或输入缺失的鲁棒性**。\n",
    "\n",
    "**稀疏自编码器（Sparse Autoencoders）**：训练时结合编码层的稀疏惩罚$\\Omega(h)$和重构误差：$L(x,g(f(x)))+Ω(h)$。 一般用来学习特征，以便用于像分类这样的任务。\n",
    "\n",
    "**惩罚导数作为正则**：有种损失函数：$L(x,g(f(x)))+Ω(h,x)$，其中，$Ω(h,x)=λ∑_i|∇_xh_i|^2$。 这迫使模型学习一个在$x$变化小时目标也没有太大变化的函数。\n",
    "\n",
    "**随机编码器和解码器（Stochastic Encoders and Decoders）**：任何潜变量模型$p_{model}(h, x)$定义一个随机编码器：$p_{encoder}(h∣x)=p_{model}(h∣x)$，以及一个随机解码器：$p_{decoder}(x∣h)=p_{model}(x∣h)$。\n",
    "![](https://github.com/applenob/reading_note/raw/master/res/stochasticAutoencoder.png)\n",
    "\n",
    "**去噪自编码器（Denoising Autoencoders，DAE）**：使用损失函数：$L(x,g(f(\\tilde x)))$，其中$\\tilde x$是被某种噪声损坏的$x$的副本。 引入损坏过程（corruption process）$C(\\tilde{x} \\mid x)$，训练过程：1.从训练数据中采一个训练样本$x$；2.从$C(\\tilde{x} \\mid x=x)$采一个损坏样本$\\tilde x$；3.将$(x, \\tilde x)$作为训练样本来估计自编码器的重构分布$p_{reconstruct} (x \\mid \\tilde x) = p_{decoder}(x \\mid h)$，其中，$h$是编码器 $f(\\tilde x)$的输出，$p_{decoder}$根据解码函数$g(h)$定义。 通常我们可以简单地对负对数似然$-\\log p_{decoder} (x \\mid h)$进行基于梯度的近似最小化，因此，我们可以认为DAE是在以下期望下进行随机梯度下降：$E_{x \\sim \\hat{p}_{data}(x)} E_{\\tilde{x} \\sim C(\\tilde{x}\\mid x)} \\log p_{decoder}(x \\mid h = f(\\tilde{x}))$，其中，$\\hat{p}_{data}(x)$是训练数据的分布。 对一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器（具有sigmoid隐藏单元和relu）的去噪训练过程，与训练带高斯可见单元的RBM无向概率模型是等价的。\n",
    "![](https://github.com/applenob/reading_note/raw/master/res/dae.png)\n",
    "\n",
    "**流形的一些重要特性**：一个重要特征是**切平面（tangent plane）**：$d$维流形上的一点$x$，切平面由能张成流形上允许变动的局部方向的$d$维基向量给出。 $n$维流形在每个点处都具有$n$维切平面。 该切平面恰好在该点接触流形，并且在该点处平行于流形表面。 它定义了为保持在流形上可以移动的方向空间。\n",
    "\n",
    "**使用自编码器学习流形**：自编码器的目标是学习流形的结构。 我们可以通过构建对数据点周围的输入扰动不敏感的重构函数，使得自编码器恢复流形结构。 ![](https://github.com/applenob/reading_note/raw/master/res/manifold.png)\n",
    "\n",
    "**收缩自编码器（Contractive Encoder）**：鼓励$f$的导数尽可能小：$Ω(h)=λ|\\frac{∂f(x)}{∂x}|_F^2$。 惩罚项$\\Omega(h)$为平方Frobenius范数（元素平方之和），作用于与编码器的函数相关偏导数的Jacobian矩阵。\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
