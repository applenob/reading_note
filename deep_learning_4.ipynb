{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Numerical Computation\n",
    "\n",
    "**数值优化（Numerical Computation）**：通常指代那些在解决数学问题时，不使用从符号表达式中直接推导出解析解，而是使用迭代更新的方式获取答案的算法。\n",
    "\n",
    "**上溢和下溢（overflow/underflow）**：数据太小或者太大，在计算机内存中无法表示。\n",
    "\n",
    "**优化问题（optimization problem）**：优化目标：最小化函数：损失函数（loss function）/ 错误函数（error function）通常上标\\*表示最优解。$x^*=argmin f(x)$\n",
    "\n",
    "**临界点（critical point）**：$f'(x)=0$的点称为临界点，一般临界点取得极大值或者极小值，否则为鞍点（saddle point）。\n",
    "\n",
    "**梯度下降（gradient descent）**：$x' = x-ε\\triangledown _xf(x)$，其中，ε是学习率。\n",
    "\n",
    "**Jacobian 矩阵（Jacobian matrix）**：\n",
    "- 如果我们有一个函数f:$\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$\n",
    "- 那么Jacobian矩阵即为：$J_{i,j} = \\frac {\\partial}{\\partial x_j}f(x)_i$。\n",
    "\n",
    "**Hessian 矩阵（Hessian matrix）**：\n",
    "- $H(f)(x)_{i,j} = \\frac {\\partial ^2}{\\partial x_i \\partial x_j}f(x)$。\n",
    "- 可以知道，Hessian矩阵是对称阵。\n",
    "\n",
    "**牛顿法（Newton's method）**：\n",
    "- 将函数用二阶的泰勒公式近似：$f(x)≈f(x^{(0)})+(x-x^{(0)})^T\\triangledown_xf(x^{(0)})+\\frac{1}{2}(x-x^{(0)})^TH(f)(x^{(0)})(x-x^{(0)})$，求解临界点$x^* = x^{(0)}-H(f)(x^{(0)})^{-1}\\triangledown_xf(x^{(0)})$。\n",
    "- 梯度下降称为“一阶优化算法”；牛顿法称为“二阶优化算法”。\n",
    "\n",
    "### 拉格朗日对偶性\n",
    "\n",
    "**原始问题**：\n",
    "- $f(x),c_i(x),h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题：\n",
    "- $\\underset{x\\in R^n}{min}f(x)\\\\s.t.\\;\\;c_i(x)\\leq 0\\;\\;i=1,...,k\\\\h_j(x)=0\\;\\;j=1,...,l$\n",
    "- 即有$k$个不等式约束：$c_i(x)$和$l$个等式约束：$h_j(x)$。\n",
    "\n",
    "**拉格朗日函数**：\n",
    "- $L(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^k\\alpha_i c_i(x)+\\sum_{j=1}^l\\beta_jh_j(x)$\n",
    "- $\\alpha_i$和$\\beta_j$，称为**拉格朗日乘子**，$\\alpha_i\\geq 0$。\n",
    "\n",
    "**拉格朗日函数的极大极小问题**：\n",
    "- 令$\\theta_P(x) = \\underset{\\alpha,\\beta}{max}\\;L(x,\\alpha,\\beta)$\n",
    "- 如果存在$x$违反了原始问题的约束条件，则$\\theta_P(x)=\\infty$，当$x$不违反原始问题的约束条件，则$\\theta_P(x)=f(x)$\n",
    "- 因此：$\\underset{x}{min}\\theta_P(x)=\\underset{x}{min}\\underset{\\alpha,\\beta}{max}L(x,\\alpha,\\beta)$等价于原问题。\n",
    "\n",
    "**对偶问题**：\n",
    "- $\\underset{\\alpha,\\beta}{max}\\underset{x}{min}\\;L(x,\\alpha,\\beta)$\n",
    "- 定理：如果函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，则$x^*,\\alpha^*,\\beta^*$是同时是原始问题和对偶问题的解的必要条件是满足KKT条件。\n",
    "\n",
    "**KKT条件**：\n",
    "- 1.拉格朗日函数对$x$，$λ$，$α$求偏导都为0：\n",
    "    - $\\triangledown_xL(x^*,\\alpha^*,\\beta^*)=0$\n",
    "    - $\\triangledown_{\\alpha}L(x^*,\\alpha^*,\\beta^*)=0$\n",
    "    - $\\triangledown_{\\beta}L(x^*,\\alpha^*,\\beta^*)=0$\n",
    "- 2.对于不等式约束，$\\alpha_i^*c_i(x^*)=0\\;\\;\\;i=1,...,k$（对偶互补条件）。\n",
    "\n",
    "****\n",
    "\n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整笔记目录\n",
    "\n",
    "- [0.书本介绍](https://applenob.github.io/deep_learning_1#0.书本介绍)\n",
    "- [1. Introduction](https://applenob.github.io/deep_learning_1#1.-Introduction)\n",
    "- [2. Linear Algebra](https://applenob.github.io/deep_learning_2)\n",
    "- [3. Probability and Information Theory](https://applenob.github.io/deep_learning_3)\n",
    "- [4. Numerical Computation](https://applenob.github.io/deep_learning_4)\n",
    "- [5. Machine Learning Basics](https://applenob.github.io/deep_learning_5)\n",
    "- [6. Deep Feedforward Networks](https://applenob.github.io/deep_learning_6)\n",
    "- [7. Regularization for Deep Learning](https://applenob.github.io/deep_learning_7)\n",
    "- [8. Optimization for Training Deep Models](https://applenob.github.io/deep_learning_8)\n",
    "- [9. Convolutional Networks](https://applenob.github.io/deep_learning_9)\n",
    "- [10. Sequence Modeling: Recurrent and Recursive Nets](https://applenob.github.io/deep_learning_10)\n",
    "- [11. Practical Methodology](https://applenob.github.io/deep_learning_11)\n",
    "- [12. Application](https://applenob.github.io/deep_learning_12)\n",
    "- [13. Linear Factor Models](https://applenob.github.io/deep_learning_13)\n",
    "- [14. Autoencoders](https://applenob.github.io/deep_learning_14)\n",
    "- [15. Representation Learning](https://applenob.github.io/deep_learning_15)\n",
    "- [16. Structured Probabilistic Models for Deep Learning](https://applenob.github.io/deep_learning_16)\n",
    "- [17. Monte Carlo Methods](https://applenob.github.io/deep_learning_17)\n",
    "- [18. Confronting the Partition Function](https://applenob.github.io/deep_learning_18)\n",
    "- [19. Approximate Inference](https://applenob.github.io/deep_learning_19)\n",
    "- [20. Deep Generative Models](https://applenob.github.io/deep_learning_20)\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
