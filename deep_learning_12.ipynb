{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [0.书本介绍](https://applenob.github.io/deep_learning_1#0.书本介绍)\n",
    "- [1. Introduction](https://applenob.github.io/deep_learning_1#1.-Introduction)\n",
    "- [2. Linear Algebra](https://applenob.github.io/deep_learning_2)\n",
    "- [3. Probability and Information Theory](https://applenob.github.io/deep_learning_3)\n",
    "- [4. Numerical Computation](https://applenob.github.io/deep_learning_4)\n",
    "- [5. Machine Learning Basics](https://applenob.github.io/deep_learning_5)\n",
    "- [6. Deep Feedforward Networks](https://applenob.github.io/deep_learning_6)\n",
    "- [7. Regularization for Deep Learning](https://applenob.github.io/deep_learning_7)\n",
    "- [8. Optimization for Training Deep Models](https://applenob.github.io/deep_learning_8)\n",
    "- [9. Convolutional Networks](https://applenob.github.io/deep_learning_9)\n",
    "- [10. Sequence Modeling: Recurrent and Recursive Nets](https://applenob.github.io/deep_learning_10)\n",
    "- [11. Practical Methodology](https://applenob.github.io/deep_learning_11)\n",
    "- [12. Application](https://applenob.github.io/deep_learning_12)\n",
    "- [13. Linear Factor Models](https://applenob.github.io/deep_learning_13)\n",
    "- [14. Autoencoders](https://applenob.github.io/deep_learning_14)\n",
    "- [15. Representation Learning](https://applenob.github.io/deep_learning_15)\n",
    "- [16. Structured Probabilistic Models for Deep Learning](https://applenob.github.io/deep_learning_16)\n",
    "- [17. Monte Carlo Methods](https://applenob.github.io/deep_learning_17)\n",
    "- [18. Confronting the Partition Function](https://applenob.github.io/deep_learning_18)\n",
    "- [19. Approximate Inference](https://applenob.github.io/deep_learning_19)\n",
    "- [20. Deep Generative Models](https://applenob.github.io/deep_learning_20)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 12. Application\n",
    "\n",
    "### 计算机视觉\n",
    "\n",
    "**计算机视觉的应用**：计算机视觉的应用广泛：从复现人类视觉能力（比如识别人脸）到创造全新的视觉能力。\n",
    "\n",
    "**预处理**：1.标准化：使得它们的像素都在相同并且合理的范围内，比如$[0,1]$或者$[-1,1]$。2.标准尺寸：裁剪或缩放图像以适应固定的尺寸。3.数据集增强：只对训练集做预处理，是减少大多数计算机视觉模型泛化误差的一种极好方法。\n",
    "\n",
    "**对比度归一化**：对比度指的是图像中亮像素和暗像素之间差异的大小，在深度学习中，对比度通常指的是图像或图像区域中像素的标准差。 **全局对比度归一化**旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数$s$来防止图像具有变化的对比度。 局部对比度归一化确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。\n",
    "\n",
    "**数据集增强**：我们很容易通过增加训练集的额外副本来增加训练集的大小，进而改进分类器的泛化能力。 这些额外副本可以通过对原始图像进行一些变化来生成，但是并不改变其类别。 对象识别这个分类任务特别适合于这种形式的数据集增强，因为类别信息对于许多变换是不变的，而我们可以简单地对输入应用诸多几何变换。 分类器可以受益于**随机转换或者旋转**。 还有一些高级的用以数据集增强的变换，包括图像中**颜色的随机扰动**，以及对输入的**非线性几何变形**。\n",
    "\n",
    "### 语音识别\n",
    "\n",
    "**语音识别**：语音识别任务旨在将一段包括了自然语言发音的声学信号投影到对应说话人的词序列上。\n",
    "\n",
    "**语音识别的应用**：1.其中的一个创新点是卷积网络的应用。 卷积网络在时域与频域上复用了权重，改进了之前的仅在时域上使用重复权值的时延神经网络。 这种新的二维的卷积模型并不是将输入的频谱当作一个长的向量，而是当成是一个图像，其中一个轴对应着时间，另一个轴对应的是谱分量的频率。2.完全抛弃HMM转向研究端到端的深度学习语音识别系统是一大活跃领域，这个领域第一个主要的突破是一个深度的LSTM神经网络\n",
    "\n",
    "### 自然语言处理\n",
    "\n",
    "**自然语言处理**：自然语言处理让计算机能够使用人类语言，例如英语或法语。\n",
    "\n",
    "**$n$-gram**：语言模型定义了自然语言中标记序列的概率分布。 根据模型的设计，标记可以是词、字符、甚至是字节。 基于$n$-gram的模型定义一个条件概率——给定前$n-1$个标记后的第$n$个标记的条件概率。 该模型使用这些条件分布的乘积定义较长序列的概率分布：$P(x_1,…,x_τ)=P(x_1,…,x_{n−1})∏_{t=n}^τP(x_t∣x_{t−n+1},…,x_{t−1})$。 训练$n$-gram模型是简单的，因为最大似然估计可以通过简单地统计每个可能的$n$-gram在训练集中出现的次数来获得。 几十年来，基于$n$-gram的模型都是统计语言模型的核心模块。 通常我们同时训练~$n$-gram~模型和$n-1$ gram模型。 这使得下式可以简单地通过查找两个存储的概率来计算：$P(x_t∣x_{t−n+1},…,x_{t−1})=P_n(x_{t−n+1},…,x_t)P_{n−1}(x_{t−n+1},…,x_{t−1})$。 $n$-gram特别容易引起维数灾难。\n",
    "\n",
    "**神经语言模型**：神经语言模型是一类用来克服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模。 不同于基于类的$n$-gram模型，神经语言模型在能够识别两个相似的词，并且不丧失将每个词编码为彼此不同的能力。 \n",
    "\n",
    "**神经机器翻译**：机器翻译以一种自然语言读取句子并产生等同含义的另一种语言的句子。 机器翻译系统通常涉及许多组件：在高层次，一个组件通常会提出许多候选翻译。 由于语言之间的差异，这些翻译中的许多翻译是不符合语法的。 翻译系统的第二个组成部分（语言模型）评估提议的翻译，并可以评估哪句翻译地更好。\n",
    "\n",
    "### 其他应用\n",
    "\n",
    "**推荐系统**：可以分为两种主要的应用：在线广告和项目建议（通常这些建议的目的仍然是为了销售产品）。 两者都依赖于预测用户和项目之间的关联， 一旦向该用户展示了广告或推荐了该产品，推荐系统要么预测一些行为的概率（用户购买产品或该行为的一些代替）或预期增益（其可取决于产品的价值）。早期推荐系统的工作依赖于这些预测输入的最小信息：用户ID和项目ID。 在这种情况下，唯一的泛化方式依赖于不同用户或不同项目的目标变量值之间的模式相似性。 假设用户1和用户2都喜欢项目A，B和C. 由此，我们可以推断出用户1和用户2具有类似的口味。 如果用户1喜欢项目D，那么这可以强烈提示用户2也喜欢D。 基于此原理的算法称为**协同过滤**。 然而，协同过滤系统有一个基本限制：当引入新项目或新用户时，缺乏评级历史意味着无法评估其与其他项目或用户的相似性，或者说无法评估新的用户和现有项目的联系。 这被称为冷启动推荐问题。 解决冷启动推荐问题的一般方式是引入单个用户和项目的额外信息。 例如，该额外信息可以是用户简要信息或每个项目的特征。 使用这种信息的系统被称为基于内容的推荐系统(content-based recommender system)。 从丰富的用户特征或项目特征集到嵌入的映射可以通过深度学习架构学习。 专用的深度学习架构，如卷积网络已经应用于从丰富内容中提取特征，如提取用于音乐推荐的音乐音轨。 在该工作中，卷积网络将声学特征作为输入并计算相关歌曲的嵌入。 该歌曲嵌入和用户嵌入之间的点积则可以预测用户是否将收听该歌曲。\n",
    "\n",
    "**知识表示、推理和回答**：数学中，二元关系是一组有序的对象对。 集合中的对具有这种关系，而那些不在集合中的对则没有。 例如，我们可以在实体集${ 1, 2, 3 }$上定义关系”小于”来定义有序对的集合$S = { (1, 2), (1, 3), (2, 3) }$。 一旦这个关系被定义，我们可以像动词一样使用它。 因为$(1, 2) \\in S$，我们说1小于2。 一种常见的引入深度学习方法是将神经语言模型扩展到模型实体和关系。 神经语言模型学习提供每个词分布式表示的向量。 他们还通过学习这些向量的函数来学习词之间的相互作用，例如哪些词可能出现在词序列之后。 我们可以学习每个关系的嵌入向量将这种方法扩展到实体和关系。\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
