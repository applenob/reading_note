{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 《Deep Learning》\n",
    "\n",
    "![](https://img3.doubanio.com/lpic/s29133163.jpg)\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [0.书本介绍](#0.书本介绍)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Linear Algebra](#2.-Linear-Algebra)\n",
    "- [3. Probability and Information Theory](3.-Probability-and-Information-Theory)\n",
    "- [4. Numerical Computation](4.-Numerical-Computation)\n",
    "- [5. Machine Learning Basics](#5.-Machine-Learning-Basics)\n",
    "- [6. Deep Feedforward Networks](#6.-Deep-Feedforward-Networks)\n",
    "- [7. Regularization for Deep Learning](#7.-Regularization-for-Deep-Learning)\n",
    "- [8. Optimization for Training Deep Models](#8.-Optimization-for-Training-Deep-Models)\n",
    "- [9. Convolutional Networks](#9.-Convolutional-Networks)\n",
    "- [10. Sequence Modeling: Recurrent and Recursive Nets](#10.-Sequence-Modeling:-Recurrent-and-Recursive-Nets)\n",
    "- [11. Practical Methodology](#11.-Practical-Methodology)\n",
    "- [12. Application](#12.-Application)\n",
    "- [13. Linear Factor Models](#13.-Linear-Factor-Models)\n",
    "- [14. Autoencoders](#14.-Autoencoders)\n",
    "- [15. Representation Learning](#15.-Representation-Learning)\n",
    "- [16. Structured Probabilistic Models for Deep Learning](#16.-Structured-Probabilistic-Models-for-Deep-Learning)\n",
    "- [17. Monte Carlo Methods](#17.-Monte-Carlo-Methods)\n",
    "- [18. Confronting the Partition Function](#18.-Confronting-the-Partition-Function)\n",
    "- [19. Approximate](#19.-Approximate)\n",
    "- [20. Deep Generative Models](#20.-Deep-Generative-Models)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 0.书本介绍\n",
    "\n",
    "[Deep Learning](https://book.douban.com/subject/26883982/)\n",
    "\n",
    "作者:  Ian Goodfellow / Yoshua Bengio / Aaron Courville \n",
    "\n",
    "内容简介：\n",
    "\n",
    "```\n",
    "\"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, co-chair of OpenAI; co-founder and CEO of Tesla and SpaceX\n",
    "Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning.\n",
    "The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.\n",
    "Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.\n",
    "```\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "什么是**machine learning**? \n",
    "\n",
    "在原始的AI系统中，定义不同的case使用不同的解决方法，这称为“hard code”。进一步的AI系统需要一种去获取知识的能力，也就是从原始数据中发现模式（“Pattern”），这种能力就是**machine learning**。\n",
    "\n",
    "但是，一般的machine learning算法严重依赖于数据的**表示(representation)**，表示中包含的每份信息又称为**feature**。\n",
    "\n",
    "这又引发了一个新的问题，对于很多task，我们不知道应该提取什么样的特征（只能经验主义）。\n",
    "\n",
    "于是又有了**representation learning**，即使用machine learning不光光是学习reprsentation到output的映射（mapping），还要学习data到representation的映射。\n",
    "\n",
    "典型的表示学习算法是**autoencoder**。encoder函数是将输入数据映射成表示;decoder函数将表示映射回原始数据的格式。\n",
    "\n",
    "representation learning的难点：表示是多种多样的，一种表示学习算法很难覆盖多种层次和不同类型的表示。\n",
    "\n",
    "**Deep Learning**：使用多层次的结构，用简单的表示来获取高层的表示。这样，解决了上面的问题（一种方法）。\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dl.png)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 2. Linear Algebra\n",
    "\n",
    "- Scalars: 一个数；\n",
    "\n",
    "- Vctors: 一列数；\n",
    "\n",
    "- Matrices: 二位数组的数，每个元素由两个下标确定；\n",
    "\n",
    "- Tensors: 多维数组的数。\n",
    "\n",
    "***\n",
    "\n",
    "**转置（transpose）**：$(A^T)_{i,j}=A_{j,i}$\n",
    "\n",
    "**矩阵乘法**: $C=AB$， $C_{i,j}=\\sum_kA_{i,k}B_{k,j}$\n",
    "\n",
    "**元素乘法(element product; Hardamard product)**：$A \\bigodot B$\n",
    "\n",
    "**点乘(dot product)**: 向量**x**，**y**的点乘：  $x^Ty$\n",
    "\n",
    "**单位矩阵(identic matrix)**: $I_n$， 斜对角的元素值是1,其他地方都是0\n",
    "\n",
    "**逆矩阵（inverse matrix）**:$A^{-1}$, $A^{-1}A=I_n$\n",
    "\n",
    "方程Ax=b，如果A可逆，则$x=A^{-1}b$\n",
    "\n",
    "**线性组合（linear combination）**：将矩阵A看作是不同的列向量的组合[d1,d2,...,dn]，每个列响亮代表一个方向，x可以代表在每个方向上移动的距离，那么Ax=b可以理解成原点如何在A指定的各个方向上移动，最后到达b点。Ax即为线性组合，组合的对象是各个列向量，方式是x的元素。\n",
    "\n",
    "**生成空间（span）**：对所有的x，生成的点Ax的集合，即为A的生成空间。\n",
    "\n",
    "**范数（Norm）**：用来衡量vector的尺寸。$L^p$ norm:\n",
    "$$||x||_p = \\left ( \\sum_i{|x_i|^p} \\right )^{\\frac{1}{p}}$$\n",
    "\n",
    "**Frobenius-norm**: 用来衡量matrix的尺寸。类似于$L_2$ norm\n",
    "$$||A||_F=\\sqrt{\\sum_{i,j}{A_{i,j}^2}}$$\n",
    "\n",
    "**对角阵（diagnal matrix）**：除了对角线上的元素不为0,其他元素都为0。可以表示为diag(v)。\n",
    "\n",
    "**对称阵（symmetric matrix）**：$A=A^T$\n",
    "\n",
    "**单位向量（unit vector）**：$||x||_2 = 1$\n",
    "\n",
    "**正交（orthogonal）**：如果$x^Ty=0$，则向量x和向量y彼此正交。\n",
    "\n",
    "**正交归一化矩阵（orthonormal matrix）**：每行都相互正交并且都是单位向量。$A^TA=AA^T=I$，有$A^{-1}=A^T$。\n",
    "\n",
    "**特征分解**：特征向量v和特征值λ，满足：$Av = λv$，方阵A可以这样分解：$A=Vdiag(λ)V^{-1}$，其中,$V=[v^{(1)},...,v^{(n)}]$,$λ=[λ_1, ..., λ_n]^T$\n",
    "\n",
    "**正定（positive definite）**：一个矩阵的所有特征值都是正的，则称这个矩阵正定。\n",
    "\n",
    "**奇异值分解（singular value decomposition）**：$A = UDV^T$，其中A是m×n矩阵；U是m×m正交矩阵，U的列向量称为左奇异向量，是$AA^T$的特征向量；D是m×n对角矩阵，对角线上的元素称为奇异值；V是正交n×n矩阵，V的列向量称为右奇异向量，是$A^TA$的特征向量。\n",
    "\n",
    "**伪逆（Moore-Penrose Pseudoinverse）**：$A^+ = VD^+U^T$，其中，$D^+$是由D的每个对角线元素取倒数（reciprocal）获得。\n",
    "\n",
    "**迹（Trace）**：$Tr(A) = \\sum_i A_{i,i} $，即对角线元素之和。\n",
    "\n",
    "**行列式（Determinant）**：det(A)，是一个将一个matrix映射到一个实数的function。行列式的值等于矩阵的所有特征值的乘积。\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Probability and Information Theory\n",
    "\n",
    "### 概率论部分\n",
    "\n",
    "**频率学派概率（frequentist probability）**：认为概率和事件发生的频率相关；**贝叶斯学派概率（Bayesian probability）**：认为概率是的对某件事发生的确定程度，可以理解成是确信的程度（degree of belief）。\n",
    "\n",
    "**随机变量（random variable）**：一个可能随机获取不同值的变量。\n",
    "\n",
    "**概率质量函数（probability mass function，PMF）**：用来描述离散随机变量的概率分布。表示为P(x)，是状态到概率的映射。\n",
    "\n",
    "**概率密度函数（probability density function，PDF）**：用来描述连续随机变量的概率分布，p(x)。\n",
    "\n",
    "**条件概率（conditional probability）**：$P(y=y|x=x) = \\frac{P(y=y, x=x)}{P(x=x)}$\n",
    "\n",
    "**条件概率的链式法则（chain rule of conditional probability）**：$P(x^{(1)}, ..., x^{(n)}) = P(x^{(1)})\\prod^n_{i=2}P(x^{(i)}|x^{(1)}, ..., x^{(i-1)})$\n",
    "\n",
    "**独立（independence）**：$\\forall x ∈ x, y ∈ y, p(x=x, y=y) = p(x=x)p(y=y)$\n",
    "\n",
    "**条件独立（conditional independence）**：$\\forall x ∈ x, y ∈ y, z ∈ z,p(x=x, y=y | z=z) = p(x=x | z=z)p(y=y | z=z)$\n",
    "\n",
    "**期望（expectation）**：期望针对某个函数f(x)，关于概率分布P(x)的平均值。对离散随机变量：$E_{x \\sim P}[f(x)] = \\sum_xP(x)f(x)$；对连续随机变量：$E_{x \\sim p}[f(x)] = \\int P(x)f(x)dx$。期望是线性的：$E_x[αf(x)+βg(x)] = αE_x[f(x)]+βE_x[f(x)]$\n",
    "\n",
    "**方差（variance）**：用来衡量从随机变量x的分布函数f(x)中采样出来的一系列值和期望的偏差。$Var(x) = E[(f(x)-E[f(x)])^2]$，方差开平方即为标准差（standard deviation）。\n",
    "\n",
    "**协方差（covariance）**：用于衡量两组值之间的线性相关程度。$Cov(f(x), g(y)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])]$。独立比协方差为0更强，因为独立还排除了非线性的相关。\n",
    "\n",
    "**贝努力分布（Bernoulli Distribution）**：随机变量只有两种可能的分布，只有一个参数：Φ，即x=1的概率。\n",
    "\n",
    "**多项式分布（Multinoulli Distribution）**随机变量有k种可能的分布，参数是一个长度为k-1的向量p。\n",
    "\n",
    "**高斯分布（Gaussian Distribution）**即正态分布（normal distribution），$\\textit{N}(x;μ,σ^2) = \\sqrt{\\frac{1}{2πσ^2}}exp(-\\frac{1}{2σ^2}(x-μ)^2)$。中心极限定理（central limit theorem）认为，大量的独立随机变量的和近似于一个高斯分布，这一点可以大量使用在应用中，我们可以认为噪声是属于正态分布的。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/gauss.PNG)\n",
    "\n",
    "**多元正态分布（multivariate normal distribution）**：给定协方差矩阵$\\mathbf{Σ}$（正定对称），$\\textit{N}(x;μ,Σ) = \\sqrt{\\frac{1}{(2π)^ndet(Σ)}}exp(-\\frac{1}{2}(x-μ)^TΣ^{-1}(x-μ))$\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/450px-MultivariateNormal.png)\n",
    "\n",
    "**指数分布（exponential distribution）**：在深度学习的有研究中，经常会用到在x=0点获得最高的概率的分布，$p(x; λ) = λ\\mathbf{1}_{x≥0}exp(-λx)$，或者：$f(x) = \\left\\{\\begin{matrix}λexp(-λx) \\;\\;\\;\\; x≥0 \\\\0 \\;\\;\\;\\; else \\end{matrix}\\right.$，其中λ > 0是分布的一个参数，常被称为率参数（rate parameter）。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/exp_dis.png)\n",
    "\n",
    "**拉普拉斯（Laplace Distribution）**：另一个可以在一个点获得比较高的概率的分布。$Laplace(x ;μ,γ) = \\frac{1}{2γ}exp(-\\frac{|x-μ|}{γ})$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/laplace.jpg)\n",
    "\n",
    "**迪拉克分布（Dirac Distribution）**：$p(x) = δ(x-μ)$，这是一个泛函数。迪拉克分布经常被用于组成经验分布（empirical distribution）：$p(x) = \\frac{1}{m}\\sum_{i=1}^m{δ(x-x^{(i)})}$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dirac.png)\n",
    "\n",
    "**逻辑斯蒂函数（logistic function）**：$σ(x) = \\frac{1}{1+exp(-x)}$，常用来生成贝努力分布的Φ参数。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/logistic.png)\n",
    "\n",
    "**softplus function**: $ζ(x) = log(1+exp(x))$，是“取正”函数的“soft”版：$x^+ = max(0, x)$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/softplus.png)\n",
    "\n",
    "**贝叶斯公式（Bayes' Rule）**：$P(x|y) = \\frac{P(x)P(y|x)}{P(y)}$\n",
    "\n",
    "### 信息论部分\n",
    "\n",
    "**信息论背后的直觉： 学习一件不太可能的事件比学习一件比较可能的事件更有信息量。**\n",
    "\n",
    "**信息（information）需要满足的三个条件**：1.比较可能发生的事件的信息量要少；2.比较不可能发生的事件的信息量要大；3.独立发生的事件之间的信息量应该是可以叠加的。\n",
    "\n",
    "**自信息（self-information）：**对事件x=x，$I(x) = -logP(x)$，满足上面三个条件，单位是nats（底为e）\n",
    "\n",
    "**香农熵（Shannon entropy）**：自信息只包含一个事件的信息，对于整个概率分布p(x)，不确定性可以这样衡量：$E_{x\\sim P}[I(x)] = -E_{x\\sim P}[logP(x)]$，也可以表示成H(P)。\n",
    "\n",
    "**KL散度（Kullback-Leibler divergence）**：衡量两个分布P(x)和Q(x)之间的差距。$D_{KL}(P||Q)=E_{x\\sim P}[log \\frac{P(x)}{Q(x)}]=E_{x\\sim P}[logP(x)-logQ(x)]$，注意$D_{KL}(P||Q)≠D_{KL}(Q||P)$\n",
    "\n",
    "**交叉熵（cross entropy）**：$H(P,Q)=H(P)+D_{KL}(P||Q)=-E_{x\\sim P}[logQ(x)]$，假设P是真实分布，Q是模型分布，那么最小化交叉熵H(P,Q)可以让模型分布逼近真实分布。\n",
    "\n",
    "### 图模型（Structured Probabilistic Models）\n",
    "\n",
    "**有向图模型（Directed Model）**：$p(x) = \\prod_i p(x_i | Pa_g(x_i))$， 其中$Pa_g(x_i)$是$x_i$的父节点。举例：\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dg.png)\n",
    "\n",
    "$P(a,b,c,d,e)=p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$\n",
    "\n",
    "**无向图模型（Undirected Model）**：所有节点都彼此联通的集合称作“团”（Clique）。$p(x)=\\frac{1}{Z}\\prod_i{Φ^{(i)}(C^{(i)})}$，其中，Φ称作facor，每个factor和一个团（clique）相对应。举例：\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/udg.png)\n",
    "\n",
    "$p(a,b,c,d,e) = \\frac{1}{Z}Φ^{(1)}(a,b,c)Φ^{(2)}(b,d)Φ^{(3)}(c,e)$\n",
    "\n",
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Numerical Computation\n",
    "\n",
    "**数值优化（Numerical Computation）**：通常指代那些在解决数学问题时，不使用从符号表达式中直接推导出解析解，而是使用迭代更新的方式获取答案的算法。\n",
    "\n",
    "**上溢和下溢（overflow/underflow）**：数据太小或者太大，在计算机内存中无法表示。\n",
    "\n",
    "**优化问题（optimization problem）**：优化目标：最小化函数：损失函数（loss function）/ 错误函数（error function）通常上标\\*表示最优解。$x^*=argmin f(x)$\n",
    "\n",
    "**临界点（critical point）**：$f'(x)=0$的点称为临界点，一般临界点取得极大值或者极小值，否则为鞍点（saddle point）。\n",
    "\n",
    "**梯度下降（gradient descent）**：$x' = x-ε\\triangledown _xf(x)$，其中，ε是学习率。\n",
    "\n",
    "**Jacobian 矩阵（Jacobian matrix）**：如果我们有一个函数f:$\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$，那么Jacobian矩阵即为：$J_{i,j} = \\frac {\\partial}{\\partial x_j}f(x)_i$。\n",
    "\n",
    "**Hessian 矩阵（Hessian matrix）**：$H(f)(x)_{i,j} = \\frac {\\partial ^2}{\\partial x_i \\partial x_j}f(x)$。可以知道，Hessian矩阵是对称阵。\n",
    "\n",
    "**牛顿法（Newton's method）**：将函数用二阶的泰勒公式近似：$f(x)≈f(x^{(0)})+(x-x^{(0)})^T\\triangledown_xf(x^{(0)})+\\frac{1}{2}(x-x^{(0)})^TH(f)(x^{(0)})(x-x^{(0)})$，求解临界点$x^* = x^{(0)}-H(f)(x^{(0)})^{-1}\\triangledown_xf(x^{(0)})$。梯度下降称为“一阶优化算法”；牛顿法称为“二阶优化算法”。\n",
    "\n",
    "**KKT方法求解约束优化（constrained optimization）问题**：约束空间$\\mathbb{S}=\\{x|\\forall i, g^{(i)}(x)=0\\, and\\, \\forall j, h^{(j)}(x)\\leq 0\\}$，g称为等式约束，h称为不等式约束。引入变量$λ_i$和$α_j$，称为KKT乘子。拉格朗日函数（generalized Lagrangian）：$L(x,λ,α)=f(x)+\\sum _iλ_ig^{(i)}(x) + \\sum _jα_jh^{(j)}(x) $，则$\\underset{x}{min}\\underset{λ}{max}\\underset{α,α\\geq 0}{max}L(x,λ,α)$等价于$\\underset{x∈\\mathbb{S}}{min}f(x)$。KKT条件：1.拉格朗日函数对x,λ,α求偏导都为0；2.对于不等式约束，$α\\bigodot h(x)=\\mathbf{0}$。\n",
    "\n",
    "****\n",
    "\n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Basics\n",
    "\n",
    "**机器学习定义**：一个计算机程序，如果它能做到在任务T中的性能P随着经验E可以提高，那就可以称它是关于某类任务T和性能衡量P，从经验E中学习。\n",
    "\n",
    "**机器学习任务（T）类别**：分类（classification）/缺失输入数据的分类（classification with missing data）/回归（regression）/转录（transciption）/机器翻译（machine translation）/结构化输出（structured output）/异常检测（anomaly detection）/合成和采样（synthesis and smapling）/缺失值填补（imputation of missing data）/去噪（denoising）/密度估计（density estimation）\n",
    "\n",
    "**机器学习的性能（P）**：P因为T的不同而不同。对于像分类/缺失输入数据的分类/转录，使用准确率（accuracy）来衡量性能；而对于密度估计，通常输出模型在一些样本上概率对数的平均值。\n",
    "\n",
    "**机器学习的经验（E）**：根据经验的不同，分为监督学习和无监督学习。监督学习：学习p(x)；无监督学习：学习p(y|x)。通常来说，无监督学习通常指代从不需要人工标注数据中提取信息。\n",
    "\n",
    "**泛化（generalization）**：在先前未观测到的输入上表现良好的能力被称为泛化。\n",
    "\n",
    "**欠拟合（underfitting）和过拟合（overfitting）**：机器学习的性能取决于两点因素：1.使训练误差更小；2.使训练误差和测试误差的差距更小。分别对应欠拟合的改善和过拟合的改善。\n",
    "\n",
    "**模型的容量（capacity）**：模型的容量是指其拟合各种函数的能力。\n",
    "\n",
    "**VC维（Vapnik-Chervonenkis dimension）**：VC维用来度量二分类器的容量。假设存在m个不同的x点的训练集，分类器可以任意地标记该m个不同的x点，VC维即m的最大可能值。[详细解释](http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/)\n",
    "\n",
    "**奥卡姆剃刀（Occam's razor）**：在同样能够解释已知观测现象的假设中，我们应该挑选”最简单”的那一个。\n",
    "\n",
    "**没有免费的午餐定理（no free lunch theorem）**：所有分类算法在分类没有见过的点的时候，他们的错误率的期望是一样的。这个定理告诉我们，必须要针对特定的任务去设计机器学习算法。\n",
    "\n",
    "**正则化（Regularization）**：正则化是指我们针对减少泛化误差而不是训练误差，在一个机器学习算法上做的任何改动。\n",
    "\n",
    "**超参数（Hyperparameters）**：超参数的值不能通过学习算法本身学习出来。\n",
    "\n",
    "**验证集（Validation Sets）**：验证集用来调超参数。\n",
    "\n",
    "**最大似然估计（Maximum Likelihood Estimation, MLE）**：参数θ的最大似然估计：$θ_{ML} = \\underset{θ}{argmax}p_{model}(\\mathbb{X};θ) = \\underset{θ}{argmax}\\prod_{i=1}^mp_{model}(x^{(i)};θ)$。对数形式：$θ_{ML} = \\underset{θ}{argmax}\\sum_{i=1}^mlogp_{model}(x^{(i)};θ)$。是一种点估计方法。\n",
    "\n",
    "**贝叶斯统计（Bayesian Statistics）**：最大似然估计是频率学派的观点，认为参数θ是固定的，但是未知；贝叶斯统计观点认为，数据集是直接观察得到的，因此数据集不是随机的，但是参数θ是一个随机变量。$p(θ|x^{(1)},x^{(2)},...,x^{(m)}) = \\frac{p(x^{(1)},x^{(2)},...,x^{(m)}|θ)}p(θ){p(x^{(1)},x^{(2)},...,x^{(m)})}$\n",
    "\n",
    "**最大后验(Maximum A Posteriori, MAP)估计**：$θ_{MAP}=\\underset{θ}{argmax}p(θ∣x)=\\underset{θ}{argmax}logp(x∣θ)+logp(θ)$。是一种点估计方法。\n",
    "\n",
    "**机器学习算法的常见组成部分**：一个数据集（dataset）+一个损失函数（cost function）+一个优化过程（optimization procedure）+一个模型（model）\n",
    "\n",
    "**维数灾难（the Curse of Dimensionality）**：当数据的维数很高时，很多机器学习问题变得相当困难。 这种现象被称为维数灾难。\n",
    "\n",
    "**流形（manifold）学习**：流形指连接在一起的区域。 数学上，它是指一组点，且每个点都有其邻域。 给定一个任意的点，其流形局部看起来像是欧几里得空间。 日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。流形学习算法通过一个假设来克服这个障碍，该假设认为$R_n$中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。我们认为在人工智能的一些场景中，如涉及到处理图像、声音或者文本时，流形假设至少是近似对的。\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Feedforward Networks\n",
    "\n",
    "**深度前馈网络（Deep Feedforward Networks）**：也被称为前馈神经网络（feedforward neural networks），或者多层感知机（multi-layer perceptrons， MLPs）是典型的深度学习模型。前馈网络的目标是去近似一个函数$f^*$。模型之所以称为前馈，是因为信息只向前流动，没有反馈的连接。\n",
    "\n",
    "**基于梯度的学习（Gradient Based Learning）**：神经网络模型和线性模型最大的区别在于神经网络的非线性使得损失函数不再是凸函数。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或~SVM~的凸优化算法那样保证全局收敛。 凸优化从任何一种初始参数出发都会收敛（理论上如此——在实践中也很鲁棒但可能会遇到数值问题）。 用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。 对于前馈神经网络，将所有的权重值初始化为小随机数是很重要的。 偏置可以初始化为零或者小的正值。 \n",
    "\n",
    "**输出单元：**\n",
    "\n",
    "**1.用于高斯输出分布的线性单元（Linear Output Units）**：$\\hat y = W^Th+b$，通常用来预测条件高斯分布：$p(y|x)=N(y;\\hat y, I)$\n",
    "\n",
    "**2.用于Bernoulli输出分布的sigmoid单元（Sigmoid Output Units）**：二分类任务，可以通过这个输出单元解决。$\\hat y = σ(w^Th+b)$，其中，σ是sigmoid函数。\n",
    "\n",
    "**3.用于 Multinoulli输出分布的softmax单元（Softmax Output Units）**：$z=W^th+b$，而$softmax(z)_i=\\frac{exp(z_i)}{\\sum_jexp(z_j)}$，如果说argmax函数返回的是一个onehot的向量，那么softmax可以理解成soft版的argmax函数。\n",
    "\n",
    "**隐藏单元：**\n",
    "\n",
    "**1.修正线性单元（Rectified Linear Units，ReLU）**：使用激活函数$g(z)=max\\{0,z\\}$，有$h=g(W^Tx+b)$。通常b的初始值选一个小正值，如0.1。这样relu起初很可能是被激活的。relu的一个缺点是它不能在激活值是0的时候，进行基于梯度的学习。因此又产生了各种变体。\n",
    "\n",
    "**2.logistic sigmoid与双曲正切函数（Hyperbolic Tangent）单元**：使用logistic sigmoid：$g(z)=σ(z)$；使用双曲正弦函数：$g(z)=tanh(z)$，其中, $tanh(z)=2σ(2z)-1$。 但是，在这两个函数的两端都很容易饱和，所以不鼓励用在隐藏单元中，一定要用可以优先选择双曲正弦函数。\n",
    "\n",
    "**通用近似性质（Universal Approximation Properties）**：一个前馈神经网络如果具有线性输出层和至少一层具有激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。 虽然具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。 在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。\n",
    "\n",
    "**MLP的深度（Depth）**：具有d个输入、深度为l、每个隐藏层具有n个单元的深度整流网络可以描述的线性区域的数量是$O(\\begin{pmatrix}\n",
    "n\\\\\n",
    "d\n",
    "\\end{pmatrix}^{d(l−1)}n^d)$,意味着，这是深度l的指数级。\n",
    "\n",
    "**后向传播算法（Back-Propagation）**：后向传播算法将偏差（cost）在网络中从后往前传播，用来计算关于cost的梯度。后向传播算法本身不是学习算法，而是学习算法，像SGD，使用后向传播算法来计算梯度。对于bp的生动理解，可以参考[知乎的这个回答](https://zhihu.com/question/27239198/answer/89853077)，“同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值”；“BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜”。\n",
    "\n",
    "**计算图（Computational Graphs）**：节点代表变量（variable）；引入操作（operation）的概念，操作是一个或者多个变量的函数，如果一个变量y是由一个对于变量x的操作得来的，那么就可以画一条有向边，从x指向y；\n",
    "\n",
    "**微积分中的链式法则（Chain Rule）**：假设y=g(x)，z=f(g(x))=f(y)，那么有$\\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx}$；进一步，如果$x∈R^m，y∈R^n，g：R^m \\rightarrow R^n，f：R^n \\rightarrow R$，有$\\frac{\\partial z}{\\partial x_i}=\\sum_j \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$，可以写成向量形式：$\\triangledown _xz(\\frac{\\partial y}{\\partial x})^T\\triangledown _yz$，其中，$\\frac{\\partial y}{\\partial x}$是n×m的g的Jacobian矩阵。\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularization for Deep Learning\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimization for Training Deep Models\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Convolutional Networks\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 10. Sequence Modeling: Recurrent and Recursive Nets\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Practical Methodology\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Application\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Linear Factor Models\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Autoencoders\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Representation Learning\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Structured Probabilistic Models for Deep Learning\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Monte Carlo Methods\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Confronting the Partition Function\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Approximate\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Deep Generative Models\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
