{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 《Deep Learning》\n",
    "\n",
    "![](https://img3.doubanio.com/lpic/s29133163.jpg)\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [0.书本介绍](#0.书本介绍)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Linear Algebra](#2.-Linear-Algebra)\n",
    "- [3. Probability and Information Theory](3.-Probability-and-Information-Theory)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 0.书本介绍\n",
    "\n",
    "[Deep Learning](https://book.douban.com/subject/26883982/)\n",
    "\n",
    "作者:  Ian Goodfellow / Yoshua Bengio / Aaron Courville \n",
    "\n",
    "内容简介：\n",
    "\n",
    "```\n",
    "\"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, co-chair of OpenAI; co-founder and CEO of Tesla and SpaceX\n",
    "Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning.\n",
    "The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.\n",
    "Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.\n",
    "```\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "什么是**machine learning**? \n",
    "\n",
    "在原始的AI系统中，定义不同的case使用不同的解决方法，这称为“hard code”。进一步的AI系统需要一种去获取知识的能力，也就是从原始数据中发现模式（“Pattern”），这种能力就是**machine learning**。\n",
    "\n",
    "但是，一般的machine learning算法严重依赖于数据的**表示(representation)**，表示中包含的每份信息又称为**feature**。\n",
    "\n",
    "这又引发了一个新的问题，对于很多task，我们不知道应该提取什么样的特征（只能经验主义）。\n",
    "\n",
    "于是又有了**representation learning**，即使用machine learning不光光是学习reprsentation到output的映射（mapping），还要学习data到representation的映射。\n",
    "\n",
    "典型的表示学习算法是**autoencoder**。encoder函数是将输入数据映射成表示;decoder函数将表示映射回原始数据的格式。\n",
    "\n",
    "representation learning的难点：表示是多种多样的，一种表示学习算法很难覆盖多种层次和不同类型的表示。\n",
    "\n",
    "**Deep Learning**：使用多层次的结构，用简单的表示来获取高层的表示。这样，解决了上面的问题（一种方法）。\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dl.png)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 2. Linear Algebra\n",
    "\n",
    "- Scalars: 一个数；\n",
    "\n",
    "- Vctors: 一列数；\n",
    "\n",
    "- Matrices: 二位数组的数，每个元素由两个下标确定；\n",
    "\n",
    "- Tensors: 多维数组的数。\n",
    "\n",
    "***\n",
    "\n",
    "**转置（transpose）**：$(A^T)_{i,j}=A_{j,i}$\n",
    "\n",
    "**矩阵乘法**: $C=AB$， $C_{i,j}=\\sum_kA_{i,k}B_{k,j}$\n",
    "\n",
    "**元素乘法(element product; Hardamard product)**：$A \\bigodot B$\n",
    "\n",
    "**点乘(dot product)**: 向量**x**，**y**的点乘：  $x^Ty$\n",
    "\n",
    "**单位矩阵(identic matrix)**: $I_n$， 斜对角的元素值是1,其他地方都是0\n",
    "\n",
    "**逆矩阵（inverse matrix）**:$A^{-1}$, $A^{-1}A=I_n$\n",
    "\n",
    "方程Ax=b，如果A可逆，则$x=A^{-1}b$\n",
    "\n",
    "**线性组合（linear combination）**：将矩阵A看作是不同的列向量的组合[d1,d2,...,dn]，每个列响亮代表一个方向，x可以代表在每个方向上移动的距离，那么Ax=b可以理解成原点如何在A指定的各个方向上移动，最后到达b点。Ax即为线性组合，组合的对象是各个列向量，方式是x的元素。\n",
    "\n",
    "**生成空间（span）**：对所有的x，生成的点Ax的集合，即为A的生成空间。\n",
    "\n",
    "**范数（Norm）**：用来衡量vector的尺寸。$L^p$ norm:\n",
    "$$||x||_p = \\left ( \\sum_i{|x_i|^p} \\right )^{\\frac{1}{p}}$$\n",
    "\n",
    "**Frobenius-norm**: 用来衡量matrix的尺寸。类似于$L_2$ norm\n",
    "$$||A||_F=\\sqrt{\\sum_{i,j}{A_{i,j}^2}}$$\n",
    "\n",
    "**对角阵（diagnal matrix）**：除了对角线上的元素不为0,其他元素都为0。可以表示为diag(v)。\n",
    "\n",
    "**对称阵（symmetric matrix）**：$A=A^T$\n",
    "\n",
    "**单位向量（unit vector）**：$||x||_2 = 1$\n",
    "\n",
    "**正交（orthogonal）**：如果$x^Ty=0$，则向量x和向量y彼此正交。\n",
    "\n",
    "**正交归一化矩阵（orthonormal matrix）**：每行都相互正交并且都是单位向量。$A^TA=AA^T=I$，有$A^{-1}=A^T$。\n",
    "\n",
    "**特征分解**：特征向量v和特征值λ，满足：$Av = λv$，方阵A可以这样分解：$A=Vdiag(λ)V^{-1}$，其中,$V=[v^{(1)},...,v^{(n)}]$,$λ=[λ_1, ..., λ_n]^T$\n",
    "\n",
    "**正定（positive definite）**：一个矩阵的所有特征值都是正的，则称这个矩阵正定。\n",
    "\n",
    "**奇异值分解（singular value decomposition）**：$A = UDV^T$，其中A是m×n矩阵，A是m×n矩阵，U是m×m正交矩阵，U的列向量称为左奇异向量，D是m×n对角矩阵，对角线上的元素称为奇异值，V是正交n×n矩阵，V的列向量称为右奇异向量。\n",
    "\n",
    "**伪逆（Moore-Penrose Pseudoinverse）**：$A^+ = VD^+U^T$，其中，$D^+$是由D的每个对角线元素取倒数（reciprocal）获得。\n",
    "\n",
    "**迹（Trace）**：$Tr(A) = \\sum_i A_{i,i} $，即对角线元素之和。\n",
    "\n",
    "**行列式（Determinant）**：det(A)，是一个将一个matrix映射到一个实数的function。行列式的值等于矩阵的所有特征值的乘积。\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Probability and Information Theory\n",
    "\n",
    "**频率学派概率（frequentist probability）**：认为概率和发生的频率相关；**贝叶斯学派概率（Bayesian probability）**：认为概率是的对某件事发生的确定程度，可以理解成是确信的程度（degree of belief）。\n",
    "\n",
    "**随机变量（random variable）**：一个可能随机获取不同值的变量。\n",
    "\n",
    "**概率质量函数（probability mass function，PMF）**：用来描述离散随机变量的概率分布。表示为P(x)，是状态到概率的映射。\n",
    "\n",
    "**概率密度函数（probability density function，PDF）**：用来描述连续随机变量的概率分布，p(x)。\n",
    "\n",
    "**条件概率（conditional probability）**：$P(y=y|x=x) = \\frac{P(y=y, x=x)}{P(x=x)}$\n",
    "\n",
    "**条件概率的链式法则（chain rule of conditional probability）**：$P(x^{(1)}, ..., x^{(n)}) = P(x^{(1)})\\prod^n_{i=2}P(x^{(i)}|x^{(1)}, ..., x^{(i-1)})$\n",
    "\n",
    "**独立（independence）**：$\\forall x \\epsilon x, y \\epsilon y, p(x=x, y=y) = p(x=x)p(y=y)$\n",
    "\n",
    "**条件独立（conditional independence）**：$\\forall x \\epsilon x, y \\epsilon y, z \\epsilon z,p(x=x, y=y | z=z) = p(x=x | z=z)p(y=y | z=z)$\n",
    "\n",
    "**期望（expectation）**：期望针对某个函数f(x)，关于概率分布P(x)的平均值。对离散随机变量：$E_{x \\sim P}[f(x)] = \\sum_xP(x)f(x)$；对连续随机变量：$E_{x \\sim p}[f(x)] = \\int P(x)f(x)dx$。\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
