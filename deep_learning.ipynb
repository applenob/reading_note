{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 《Deep Learning》\n",
    "\n",
    "![](https://img3.doubanio.com/lpic/s29133163.jpg)\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [0.书本介绍](#0.书本介绍)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Linear Algebra](#2.-Linear-Algebra)\n",
    "- [3. Probability and Information Theory](3.-Probability-and-Information-Theory)\n",
    "- [4. Numerical Computation](4.-Numerical-Computation)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 0.书本介绍\n",
    "\n",
    "[Deep Learning](https://book.douban.com/subject/26883982/)\n",
    "\n",
    "作者:  Ian Goodfellow / Yoshua Bengio / Aaron Courville \n",
    "\n",
    "内容简介：\n",
    "\n",
    "```\n",
    "\"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, co-chair of OpenAI; co-founder and CEO of Tesla and SpaceX\n",
    "Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning.\n",
    "The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.\n",
    "Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.\n",
    "```\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "什么是**machine learning**? \n",
    "\n",
    "在原始的AI系统中，定义不同的case使用不同的解决方法，这称为“hard code”。进一步的AI系统需要一种去获取知识的能力，也就是从原始数据中发现模式（“Pattern”），这种能力就是**machine learning**。\n",
    "\n",
    "但是，一般的machine learning算法严重依赖于数据的**表示(representation)**，表示中包含的每份信息又称为**feature**。\n",
    "\n",
    "这又引发了一个新的问题，对于很多task，我们不知道应该提取什么样的特征（只能经验主义）。\n",
    "\n",
    "于是又有了**representation learning**，即使用machine learning不光光是学习reprsentation到output的映射（mapping），还要学习data到representation的映射。\n",
    "\n",
    "典型的表示学习算法是**autoencoder**。encoder函数是将输入数据映射成表示;decoder函数将表示映射回原始数据的格式。\n",
    "\n",
    "representation learning的难点：表示是多种多样的，一种表示学习算法很难覆盖多种层次和不同类型的表示。\n",
    "\n",
    "**Deep Learning**：使用多层次的结构，用简单的表示来获取高层的表示。这样，解决了上面的问题（一种方法）。\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dl.png)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## 2. Linear Algebra\n",
    "\n",
    "- Scalars: 一个数；\n",
    "\n",
    "- Vctors: 一列数；\n",
    "\n",
    "- Matrices: 二位数组的数，每个元素由两个下标确定；\n",
    "\n",
    "- Tensors: 多维数组的数。\n",
    "\n",
    "***\n",
    "\n",
    "**转置（transpose）**：$(A^T)_{i,j}=A_{j,i}$\n",
    "\n",
    "**矩阵乘法**: $C=AB$， $C_{i,j}=\\sum_kA_{i,k}B_{k,j}$\n",
    "\n",
    "**元素乘法(element product; Hardamard product)**：$A \\bigodot B$\n",
    "\n",
    "**点乘(dot product)**: 向量**x**，**y**的点乘：  $x^Ty$\n",
    "\n",
    "**单位矩阵(identic matrix)**: $I_n$， 斜对角的元素值是1,其他地方都是0\n",
    "\n",
    "**逆矩阵（inverse matrix）**:$A^{-1}$, $A^{-1}A=I_n$\n",
    "\n",
    "方程Ax=b，如果A可逆，则$x=A^{-1}b$\n",
    "\n",
    "**线性组合（linear combination）**：将矩阵A看作是不同的列向量的组合[d1,d2,...,dn]，每个列响亮代表一个方向，x可以代表在每个方向上移动的距离，那么Ax=b可以理解成原点如何在A指定的各个方向上移动，最后到达b点。Ax即为线性组合，组合的对象是各个列向量，方式是x的元素。\n",
    "\n",
    "**生成空间（span）**：对所有的x，生成的点Ax的集合，即为A的生成空间。\n",
    "\n",
    "**范数（Norm）**：用来衡量vector的尺寸。$L^p$ norm:\n",
    "$$||x||_p = \\left ( \\sum_i{|x_i|^p} \\right )^{\\frac{1}{p}}$$\n",
    "\n",
    "**Frobenius-norm**: 用来衡量matrix的尺寸。类似于$L_2$ norm\n",
    "$$||A||_F=\\sqrt{\\sum_{i,j}{A_{i,j}^2}}$$\n",
    "\n",
    "**对角阵（diagnal matrix）**：除了对角线上的元素不为0,其他元素都为0。可以表示为diag(v)。\n",
    "\n",
    "**对称阵（symmetric matrix）**：$A=A^T$\n",
    "\n",
    "**单位向量（unit vector）**：$||x||_2 = 1$\n",
    "\n",
    "**正交（orthogonal）**：如果$x^Ty=0$，则向量x和向量y彼此正交。\n",
    "\n",
    "**正交归一化矩阵（orthonormal matrix）**：每行都相互正交并且都是单位向量。$A^TA=AA^T=I$，有$A^{-1}=A^T$。\n",
    "\n",
    "**特征分解**：特征向量v和特征值λ，满足：$Av = λv$，方阵A可以这样分解：$A=Vdiag(λ)V^{-1}$，其中,$V=[v^{(1)},...,v^{(n)}]$,$λ=[λ_1, ..., λ_n]^T$\n",
    "\n",
    "**正定（positive definite）**：一个矩阵的所有特征值都是正的，则称这个矩阵正定。\n",
    "\n",
    "**奇异值分解（singular value decomposition）**：$A = UDV^T$，其中A是m×n矩阵；U是m×m正交矩阵，U的列向量称为左奇异向量，是$AA^T$的特征向量；D是m×n对角矩阵，对角线上的元素称为奇异值；V是正交n×n矩阵，V的列向量称为右奇异向量，是$A^TA$的特征向量。\n",
    "\n",
    "**伪逆（Moore-Penrose Pseudoinverse）**：$A^+ = VD^+U^T$，其中，$D^+$是由D的每个对角线元素取倒数（reciprocal）获得。\n",
    "\n",
    "**迹（Trace）**：$Tr(A) = \\sum_i A_{i,i} $，即对角线元素之和。\n",
    "\n",
    "**行列式（Determinant）**：det(A)，是一个将一个matrix映射到一个实数的function。行列式的值等于矩阵的所有特征值的乘积。\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Probability and Information Theory\n",
    "\n",
    "### 概率论部分\n",
    "\n",
    "**频率学派概率（frequentist probability）**：认为概率和事件发生的频率相关；**贝叶斯学派概率（Bayesian probability）**：认为概率是的对某件事发生的确定程度，可以理解成是确信的程度（degree of belief）。\n",
    "\n",
    "**随机变量（random variable）**：一个可能随机获取不同值的变量。\n",
    "\n",
    "**概率质量函数（probability mass function，PMF）**：用来描述离散随机变量的概率分布。表示为P(x)，是状态到概率的映射。\n",
    "\n",
    "**概率密度函数（probability density function，PDF）**：用来描述连续随机变量的概率分布，p(x)。\n",
    "\n",
    "**条件概率（conditional probability）**：$P(y=y|x=x) = \\frac{P(y=y, x=x)}{P(x=x)}$\n",
    "\n",
    "**条件概率的链式法则（chain rule of conditional probability）**：$P(x^{(1)}, ..., x^{(n)}) = P(x^{(1)})\\prod^n_{i=2}P(x^{(i)}|x^{(1)}, ..., x^{(i-1)})$\n",
    "\n",
    "**独立（independence）**：$\\forall x ∈ x, y ∈ y, p(x=x, y=y) = p(x=x)p(y=y)$\n",
    "\n",
    "**条件独立（conditional independence）**：$\\forall x ∈ x, y ∈ y, z ∈ z,p(x=x, y=y | z=z) = p(x=x | z=z)p(y=y | z=z)$\n",
    "\n",
    "**期望（expectation）**：期望针对某个函数f(x)，关于概率分布P(x)的平均值。对离散随机变量：$E_{x \\sim P}[f(x)] = \\sum_xP(x)f(x)$；对连续随机变量：$E_{x \\sim p}[f(x)] = \\int P(x)f(x)dx$。期望是线性的：$E_x[αf(x)+βg(x)] = αE_x[f(x)]+βE_x[f(x)]$\n",
    "\n",
    "**方差（variance）**：用来衡量从随机变量x的分布函数f(x)中采样出来的一系列值和期望的偏差。$Var(x) = E[(f(x)-E[f(x)])^2]$，方差开平方即为标准差（standard deviation）。\n",
    "\n",
    "**协方差（covariance）**：用于衡量两组值之间的线性相关程度。$Cov(f(x), g(y)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])]$。独立比协方差为0更强，因为独立还排除了非线性的相关。\n",
    "\n",
    "**贝努力分布（Bernoulli Distribution）**：随机变量只有两种可能的分布，只有一个参数：Φ，即x=1的概率。\n",
    "\n",
    "**多项式分布（Multinoulli Distribution）**随机变量有k种可能的分布，参数是一个长度为k-1的向量p。\n",
    "\n",
    "**高斯分布（Gaussian Distribution）**即正态分布（normal distribution），$\\textit{N}(x;μ,σ^2) = \\sqrt{\\frac{1}{2πσ^2}}exp(-\\frac{1}{2σ^2}(x-μ)^2)$。中心极限定理（central limit theorem）认为，大量的独立随机变量的和近似于一个高斯分布，这一点可以大量使用在应用中，我们可以认为噪声是属于正态分布的。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/gauss.PNG)\n",
    "\n",
    "**多元正态分布（multivariate normal distribution）**：给定协方差矩阵$\\mathbf{Σ}$（正定对称），$\\textit{N}(x;μ,Σ) = \\sqrt{\\frac{1}{(2π)^ndet(Σ)}}exp(-\\frac{1}{2}(x-μ)^TΣ^{-1}(x-μ))$\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/450px-MultivariateNormal.png)\n",
    "\n",
    "**指数分布（exponential distribution）**：在深度学习的有研究中，经常会用到在x=0点获得最高的概率的分布，$p(x; λ) = λ\\mathbf{1}_{x≥0}exp(-λx)$，或者：$f(x) = \\left\\{\\begin{matrix}λexp(-λx) \\;\\;\\;\\; x≥0 \\\\0 \\;\\;\\;\\; else \\end{matrix}\\right.$，其中λ > 0是分布的一个参数，常被称为率参数（rate parameter）。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/exp_dis.png)\n",
    "\n",
    "**拉普拉斯（Laplace Distribution）**：另一个可以在一个点获得比较高的概率的分布。$Laplace(x ;μ,γ) = \\frac{1}{2γ}exp(-\\frac{|x-μ|}{γ})$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/laplace.jpg)\n",
    "\n",
    "**迪拉克分布（Dirac Distribution）**：$p(x) = δ(x-μ)$，这是一个泛函数。迪拉克分布经常被用于组成经验分布（empirical distribution）：$p(x) = \\frac{1}{m}\\sum_{i=1}^m{δ(x-x^{(i)})}$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dirac.png)\n",
    "\n",
    "**逻辑斯蒂函数（logistic function）**：$σ(x) = \\frac{1}{1+exp(-x)}$，常用来生成贝努力分布的Φ参数。\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/logistic.png)\n",
    "\n",
    "**softplus function**: $ζ(x) = log(1+exp(x))$，是“取正”函数的“soft”版：$x^+ = max(0, x)$\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/softplus.png)\n",
    "\n",
    "**贝叶斯公式（Bayes' Rule）**：$P(x|y) = \\frac{P(x)P(y|x)}{P(y)}$\n",
    "\n",
    "### 信息论部分\n",
    "\n",
    "**信息论背后的直觉： 学习一件不太可能的事件比学习一件比较可能的事件更有信息量。**\n",
    "\n",
    "**信息（information）需要满足的三个条件**：1.比较可能发生的事件的信息量要少；2.比较不可能发生的事件的信息量要大；3.独立发生的事件之间的信息量应该是可以叠加的。\n",
    "\n",
    "**自信息（self-information）：**对事件x=x，$I(x) = -logP(x)$，满足上面三个条件，单位是nats（底为e）\n",
    "\n",
    "**香农熵（Shannon entropy）**：自信息只包含一个事件的信息，对于整个概率分布p(x)，不确定性可以这样衡量：$E_{x\\sim P}[I(x)] = -E_{x\\sim P}[logP(x)]$，也可以表示成H(P)。\n",
    "\n",
    "**KL散度（Kullback-Leibler divergence）**：衡量两个分布P(x)和Q(x)之间的差距。$D_{KL}(P||Q)=E_{x\\sim P}[log \\frac{P(x)}{Q(x)}]=E_{x\\sim P}[logP(x)-logQ(x)]$，注意$D_{KL}(P||Q)≠D_{KL}(Q||P)$\n",
    "\n",
    "**交叉熵（cross entropy）**：$H(P,Q)=H(P)+D_{KL}(P||Q)=-E_{x\\sim P}[logQ(x)]$，假设P是真实分布，Q是模型分布，那么最小化交叉熵H(P,Q)可以让模型分布逼近真实分布。\n",
    "\n",
    "### 图模型（Structured Probabilistic Models）\n",
    "\n",
    "**有向图模型（Directed Model）**：$p(x) = \\prod_i p(x_i | Pa_g(x_i))$， 其中$Pa_g(x_i)$是$x_i$的父节点。举例：\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/dg.png)\n",
    "\n",
    "$P(a,b,c,d,e)=p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$\n",
    "\n",
    "**无向图模型（Undirected Model）**：所有节点都彼此联通的集合称作“团”（Clique）。$p(x)=\\frac{1}{Z}\\prod_i{Φ^{(i)}(C^{(i)})}$，其中，Φ称作facor，每个factor和一个团（clique）相对应。举例：\n",
    "![](https://raw.githubusercontent.com/applenob/reading_note/master/res/udg.png)\n",
    "\n",
    "$p(a,b,c,d,e) = \\frac{1}{Z}Φ^{(1)}(a,b,c)Φ^{(2)}(b,d)Φ^{(3)}(c,e)$\n",
    "\n",
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Numerical Computation\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
