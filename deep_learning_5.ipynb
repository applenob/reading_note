{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- [0.书本介绍](https://applenob.github.io/deep_learning_1#0.书本介绍)\n",
    "- [1. Introduction](https://applenob.github.io/deep_learning_1#1.-Introduction)\n",
    "- [2. Linear Algebra](https://applenob.github.io/deep_learning_2)\n",
    "- [3. Probability and Information Theory](https://applenob.github.io/deep_learning_3)\n",
    "- [4. Numerical Computation](https://applenob.github.io/deep_learning_4)\n",
    "- [5. Machine Learning Basics](https://applenob.github.io/deep_learning_5)\n",
    "- [6. Deep Feedforward Networks](https://applenob.github.io/deep_learning_6)\n",
    "- [7. Regularization for Deep Learning](https://applenob.github.io/deep_learning_7)\n",
    "- [8. Optimization for Training Deep Models](https://applenob.github.io/deep_learning_8)\n",
    "- [9. Convolutional Networks](https://applenob.github.io/deep_learning_9)\n",
    "- [10. Sequence Modeling: Recurrent and Recursive Nets](https://applenob.github.io/deep_learning_10)\n",
    "- [11. Practical Methodology](https://applenob.github.io/deep_learning_11)\n",
    "- [12. Application](https://applenob.github.io/deep_learning_12)\n",
    "- [13. Linear Factor Models](https://applenob.github.io/deep_learning_13)\n",
    "- [14. Autoencoders](https://applenob.github.io/deep_learning_14)\n",
    "- [15. Representation Learning](https://applenob.github.io/deep_learning_15)\n",
    "- [16. Structured Probabilistic Models for Deep Learning](https://applenob.github.io/deep_learning_16)\n",
    "- [17. Monte Carlo Methods](https://applenob.github.io/deep_learning_17)\n",
    "- [18. Confronting the Partition Function](https://applenob.github.io/deep_learning_18)\n",
    "- [19. Approximate Inference](https://applenob.github.io/deep_learning_19)\n",
    "- [20. Deep Generative Models](https://applenob.github.io/deep_learning_20)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Machine Learning Basics\n",
    "\n",
    "**机器学习定义**：一个计算机程序，如果它能做到在任务T中的性能P随着经验E可以提高，那就可以称它是关于某类任务T和性能衡量P，从经验E中学习。\n",
    "\n",
    "**机器学习任务（$T$）类别**：分类（classification）/缺失输入数据的分类（classification with missing data）/回归（regression）/转录（transciption）/机器翻译（machine translation）/结构化输出（structured output）/异常检测（anomaly detection）/合成和采样（synthesis and smapling）/缺失值填补（imputation of missing data）/去噪（denoising）/密度估计（density estimation）\n",
    "\n",
    "**机器学习的性能（$P$）**：$P$因为$T$的不同而不同。对于像分类/缺失输入数据的分类/转录，使用准确率（accuracy）来衡量性能；而对于密度估计，通常输出模型在一些样本上概率对数的平均值。\n",
    "\n",
    "**机器学习的经验（$E$）**：根据经验的不同，分为监督学习和无监督学习。监督学习：学习$p(x)$；无监督学习：学习$p(y|x)$。通常来说，无监督学习通常指代从不需要人工标注数据中提取信息。\n",
    "\n",
    "**泛化（generalization）**：在先前未观测到的输入上表现良好的能力被称为泛化。\n",
    "\n",
    "**欠拟合（underfitting）和过拟合（overfitting）**：机器学习的性能取决于两点因素：1.使训练误差更小；2.使训练误差和测试误差的差距更小。分别对应欠拟合的改善和过拟合的改善。\n",
    "\n",
    "**模型的容量（capacity）**：模型的容量是指其拟合各种函数的能力。\n",
    "\n",
    "**VC维（Vapnik-Chervonenkis dimension）**：VC维用来度量二分类器的容量。假设存在$m$个不同的$x$点的训练集，分类器可以任意地标记该$m$个不同的$x$点，VC维即$m$的最大可能值。[详细解释](http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/)\n",
    "\n",
    "**奥卡姆剃刀（Occam's razor）**：在同样能够解释已知观测现象的假设中，我们应该挑选”最简单”的那一个。\n",
    "\n",
    "**没有免费的午餐定理（no free lunch theorem）**：所有分类算法在分类没有见过的点的时候，他们的错误率的期望是一样的。这个定理告诉我们，必须要针对特定的任务去设计机器学习算法。\n",
    "\n",
    "**正则化（Regularization）**：正则化是指我们针对减少泛化误差而不是训练误差，在一个机器学习算法上做的任何改动。\n",
    "\n",
    "**超参数（Hyperparameters）**：超参数的值不能通过学习算法本身学习出来。\n",
    "\n",
    "**验证集（Validation Sets）**：验证集用来调超参数。\n",
    "\n",
    "### 统计学的一些基本概念：估计（Estimators）/偏差（Bias）/方差（Variance）\n",
    "\n",
    "**点估计（Point Estimation）**：\n",
    "\n",
    "试图为某些参数提供一个“最优”的预测。\n",
    "\n",
    "将参数$θ$的点估计记为$\\hat θ$，令$\\{x^{(1)}, . . . , x^{(m)}\\}$是$m$个独立同分布(i.i.d.)的数据点。点估计是这些数据的任意函数：$\\hat θ=g(x^{(1)}, . . . , x^{(m)})$\n",
    "\n",
    "**估计的偏差（Bias）**：\n",
    "\n",
    "估计的偏差被定义为:$bias(\\hat θ_m) = E(\\hat θ_m) - θ$，即，估计的期望和真实值的差。\n",
    "\n",
    "如果$bias(\\hat θ_m)=0$,那么估计量$θ_m$被称为是无偏估计。\n",
    " \n",
    "**估计的方差（Variance）**：\n",
    "\n",
    "就是一个方差$Var(\\hat θ)$。\n",
    "\n",
    "$Var(X) = E[(X - \\mu)^2]$，其中$μ=E[X]$。\n",
    "\n",
    "**均方误差（mean squared error， MSE）**：\n",
    "\n",
    "$MSE = E[(\\hat θ_m - θ)^2] = Bias(\\hat θ_m)^2 + Var(\\hat θ_m)$\n",
    "\n",
    "### 最大似然估计（Maximum Likelihood Estimation, MLE）\n",
    "\n",
    "参数θ的最大似然估计：$θ_{ML} = \\underset{θ}{argmax}\\;p_{model}(\\mathbb{X};θ) = \\underset{θ}{argmax}\\;\\prod_{i=1}^mp_{model}(x^{(i)};θ)$，其中$\\mathbb{X}=\\{x^{(1)}, . . . , x^{(m)}\\}$。\n",
    "\n",
    "似然函数：$p_{model}(\\mathbb{X};θ)$是一族由$θ$确定在相同空间上的概率分布。可以看到，**这里$θ$并不是一个随机变量，而仅仅是一个参数。**\n",
    "\n",
    "对数形式：$θ_{ML} = \\underset{θ}{argmax}\\sum_{i=1}^mlogp_{model}(x^{(i)};θ)$。\n",
    "\n",
    "是一种点估计方法。\n",
    "\n",
    "### 贝叶斯统计（Bayesian Statistics）\n",
    "\n",
    "最大似然估计是频率学派的观点，认为参数θ是固定的，但是未知；贝叶斯统计观点认为，数据集是直接观察得到的，因此数据集不是随机的，但是**参数θ是一个随机变量**。\n",
    "\n",
    "在观察到数据前,我们将$θ$的已知知识表示成**先验概率分布(prior probability distribution)**$p(θ)$。\n",
    "\n",
    "$p(θ|x^{(1)},x^{(2)},...,x^{(m)}) = \\frac{p(x^{(1)},x^{(2)},...,x^{(m)}|θ)p(θ)}{p(x^{(1)},x^{(2)},...,x^{(m)})}$\n",
    "\n",
    "### 最大后验(Maximum A Posteriori, MAP)估计\n",
    "\n",
    "$θ_{MAP}=\\underset{θ}{argmax}\\;p(θ∣x)=\\underset{θ}{argmax}\\;logp(x∣θ)+logp(θ)$\n",
    "\n",
    "我们可以认出上式右边的$logp(x|θ)$对应着标准的对数似然项，$logp(θ)$对应着先验分布。\n",
    "\n",
    "正如全贝叶斯推断,MAP 贝叶斯推断的优势是能够利用来自先验的信息,这些信息无法从训练数据中获得。该附加信息有助于减少最大后验点估计的方差，然而,这个优点的代价是增加了偏差。\n",
    "\n",
    "依然是一种点估计方法。\n",
    "\n",
    "**机器学习算法的常见组成部分**：一个数据集（dataset）+一个损失函数（cost function）+一个优化过程（optimization procedure）+一个模型（model）\n",
    "\n",
    "**维数灾难（the Curse of Dimensionality）**：当数据的维数很高时，很多机器学习问题变得相当困难。 这种现象被称为维数灾难。\n",
    "\n",
    "### 流形（manifold）学习\n",
    "\n",
    "流形指连接在一起的区域。 数学上，它是指一组点，且每个点都有其邻域。 给定一个任意的点，其流形局部看起来像是欧几里得空间。 \n",
    "\n",
    "日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。 \n",
    "\n",
    "流形学习算法通过一个假设来克服这个障碍，该假设认为$R_n$中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。\n",
    "\n",
    "我们认为在人工智能的一些场景中，如涉及到处理图像、声音或者文本时，流形假设至少是近似对的。 \n",
    "\n",
    "关于流形学习有一个很好的[中文ppt](http://www.cad.zju.edu.cn/reports/%C1%F7%D0%CE%D1%A7%CF%B0.pdf)，可以作为参考材料。\n",
    "\n",
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
